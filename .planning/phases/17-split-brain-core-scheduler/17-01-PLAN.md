---
phase: 17-split-brain-core-scheduler
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/server/database.py
  - backend/brain/syllabus_parser.py
  - backend/exams/routes.py
autonomous: true
requirements:
  - SB-02
  - SB-03

must_haves:
  truths:
    - "DB has focus_score and dependency_id columns on tasks table"
    - "DB has extracted_text column on exam_files table"
    - "exam_files CHECK constraint accepts summary and sample_exam file types"
    - "PDF uploads extract full text (all pages) and store in extracted_text column"
    - "Syllabus parser extracts all pages, not just first 5"
  artifacts:
    - path: "backend/server/database.py"
      provides: "DB migrations for new columns and updated CHECK constraint"
      contains: "focus_score"
    - path: "backend/brain/syllabus_parser.py"
      provides: "Full PDF text extraction without page cap"
    - path: "backend/exams/routes.py"
      provides: "Upload handler that saves extracted_text at upload time"
  key_links:
    - from: "backend/exams/routes.py"
      to: "backend/server/database.py"
      via: "INSERT with extracted_text column"
      pattern: "extracted_text"
---

<objective>
Database migrations, full PDF extraction, and upload handler update for the Split-Brain architecture foundation.

Purpose: All subsequent plans depend on these DB columns and the extracted_text pipeline being in place. This is the foundational data layer.
Output: Updated database schema, uncapped PDF extraction, upload handler persisting extracted text.
</objective>

<execution_context>
@/Users/eyalatar/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eyalatar/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-split-brain-core-scheduler/17-RESEARCH.md
@backend/server/database.py
@backend/brain/syllabus_parser.py
@backend/exams/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database migrations for Split-Brain columns and file type constraint</name>
  <files>backend/server/database.py</files>
  <action>
In `init_db()`, add migrations following the existing pattern (check column existence via PRAGMA table_info before ALTER TABLE):

1. Add `focus_score INTEGER DEFAULT 5` to `tasks` table (concentration level 1-10).
2. Add `dependency_id INTEGER` to `tasks` table (optional reference to prerequisite task ID).
3. Add `extracted_text TEXT` to `exam_files` table (full PDF text stored at upload time).
4. Add `auditor_draft TEXT` to `exams` table (stores Auditor JSON output for the intermediate review page, per Research recommendation Option A).
5. Update `exam_files` table CHECK constraint on `file_type` to include `'summary'` and `'sample_exam'` in addition to existing values (`'syllabus', 'past_exam', 'notes', 'other'`). Use the existing table-rebuild migration pattern (CREATE TABLE new, INSERT INTO new SELECT FROM old, DROP old, ALTER TABLE new RENAME) already present in `database.py` lines ~169-214.

Important: The table rebuild for the CHECK constraint must preserve all existing data and columns including any previously-added migration columns like `push_notified` on `schedule_blocks`. Read existing column names dynamically before rebuilding.
  </action>
  <verify>
Run the backend server (`python -m uvicorn backend.main:app`) and confirm it starts without errors. Verify via SQLite CLI:
```
sqlite3 studyflow.db "PRAGMA table_info(tasks)" | grep focus_score
sqlite3 studyflow.db "PRAGMA table_info(tasks)" | grep dependency_id
sqlite3 studyflow.db "PRAGMA table_info(exam_files)" | grep extracted_text
sqlite3 studyflow.db "PRAGMA table_info(exams)" | grep auditor_draft
```
  </verify>
  <done>All five columns exist. The exam_files CHECK constraint accepts 'summary' and 'sample_exam'. Server starts cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Remove PDF page cap and update upload handler to persist extracted text</name>
  <files>backend/brain/syllabus_parser.py, backend/exams/routes.py</files>
  <action>
1. In `syllabus_parser.py`, change `extract_text_from_pdf(pdf_bytes, max_pages=5)` to `extract_text_from_pdf(pdf_bytes, max_pages=None)`. Update the loop to skip the `max_pages` check when it's None:
   ```python
   for i, page in enumerate(doc):
       if max_pages is not None and i >= max_pages:
           break
       text += page.get_text() + "\n"
   ```

2. In `exam_brain.py`, find any call to `extract_text_from_pdf` or similar that has a page cap (the research mentions `max_pages=10` in exam_brain). Remove or set to None.

3. In `exams/routes.py` upload handler, after writing the file to disk and before the INSERT into `exam_files`:
   - For PDF files (`filename.lower().endswith('.pdf')`): open with `fitz`, extract ALL page text, join with newlines, store as `extracted_text`.
   - For non-PDF files: set `extracted_text = None`.
   - Update the INSERT statement to include the `extracted_text` column.
   - Use `fitz.open(stream=content, filetype="pdf")` where `content` is the file bytes already read.
   - Wrap in try/except so extraction failure doesn't block upload.

This ensures every PDF upload automatically has its full text stored in DB for the Auditor to read later without re-processing.
  </action>
  <verify>
Upload a multi-page PDF via the existing upload endpoint. Verify with:
```
sqlite3 studyflow.db "SELECT length(extracted_text) FROM exam_files ORDER BY id DESC LIMIT 1"
```
Result should be > 0 for a PDF upload. Server should not crash on upload of a non-PDF file.
  </verify>
  <done>PDF uploads store full extracted text in DB. Syllabus parser extracts all pages. Non-PDF uploads work without error (extracted_text is NULL).</done>
</task>

</tasks>

<verification>
1. Server starts without migration errors
2. All new columns visible in PRAGMA table_info
3. Existing data preserved after exam_files table rebuild
4. PDF upload stores extracted_text
5. Non-PDF upload succeeds with NULL extracted_text
</verification>

<success_criteria>
- Database schema updated with all 5 new columns/constraints
- Full PDF text extraction pipeline working end-to-end
- No regressions in existing upload or scheduling functionality
</success_criteria>

<output>
After completion, create `.planning/phases/17-split-brain-core-scheduler/17-01-SUMMARY.md`
</output>
