---
phase: 17-split-brain-core-scheduler
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/server/database.py
  - backend/brain/syllabus_parser.py
  - backend/exams/routes.py
autonomous: true
requirements: [SB-01, SB-02]

must_haves:
  truths:
    - "DB has focus_score and dependency_id columns on tasks table"
    - "DB has extracted_text column on exam_files table"
    - "DB has auditor_draft column on exams table"
    - "exam_files CHECK constraint accepts summary and sample_exam file types"
    - "PDF extraction reads ALL pages (no 5-page or 10K char cap)"
    - "Uploading any PDF file stores full extracted text in exam_files.extracted_text"
  artifacts:
    - path: "backend/server/database.py"
      provides: "Schema migrations for tasks, exam_files, and exams tables"
      contains: "focus_score"
    - path: "backend/brain/syllabus_parser.py"
      provides: "Uncapped PDF text extraction"
      contains: "extract_text_from_pdf"
    - path: "backend/exams/routes.py"
      provides: "Upload handler that saves extracted_text at upload time"
      contains: "extracted_text"
  key_links:
    - from: "backend/exams/routes.py"
      to: "backend/server/database.py"
      via: "INSERT with extracted_text column"
      pattern: "extracted_text"
    - from: "backend/exams/routes.py"
      to: "backend/brain/syllabus_parser.py"
      via: "fitz PDF extraction at upload time"
      pattern: "fitz\\.open|get_text"
---

<objective>
Add database schema migrations and upgrade PDF extraction to support the Split-Brain architecture.

Purpose: The Auditor and Strategist calls require focus_score, dependency_id on tasks, extracted_text on exam_files (populated at upload), and auditor_draft on exams for state persistence between the two API calls. The PDF extractor must read all pages, not just the first 5.

Output: Updated database.py with migrations, uncapped syllabus_parser.py, and upload handler that stores full extracted text.
</objective>

<execution_context>
@/Users/eyalatar/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eyalatar/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-split-brain-core-scheduler/17-RESEARCH.md
@backend/server/database.py
@backend/brain/syllabus_parser.py
@backend/exams/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database schema migrations for Split-Brain columns</name>
  <files>backend/server/database.py</files>
  <action>
In `database.py`, inside the `init_db()` function, add migrations following the existing pattern (check column existence before ALTER TABLE):

1. **tasks table** — add two columns:
   - `focus_score INTEGER DEFAULT 5` — concentration level 1-10
   - `dependency_id INTEGER` — optional FK to prerequisite task

2. **exam_files table** — add column:
   - `extracted_text TEXT` — stores full PDF text at upload time

3. **exams table** — add column:
   - `auditor_draft TEXT` — stores Auditor JSON output for the Intermediate Review Page (persists between API Call 1 and API Call 2, per CONTEXT decision on Inter-API Failure Handling)

4. **exam_files CHECK constraint update** — the existing CHECK is `CHECK(file_type IN ('syllabus', 'past_exam', 'notes', 'other'))`. Must add `'summary'` and `'sample_exam'`. Since SQLite cannot ALTER CHECK constraints, use the existing table-rebuild pattern (already present in database.py for the tasks table rebuild). Create `exam_files_new` with updated CHECK, copy data, drop old, rename new.

Use the exact migration pattern from existing code:
```python
task_columns = {row[1] for row in conn.execute("PRAGMA table_info(tasks)").fetchall()}
if "focus_score" not in task_columns:
    conn.execute("ALTER TABLE tasks ADD COLUMN focus_score INTEGER DEFAULT 5")
```

Place new migrations AFTER existing ones to avoid ordering issues.
  </action>
  <verify>
Start the backend server: `cd backend && python -m server.main` — should start without errors. Check DB schema with:
```
python -c "import sqlite3; conn=sqlite3.connect('studyflow.db'); print([r[1] for r in conn.execute('PRAGMA table_info(tasks)').fetchall()]); print([r[1] for r in conn.execute('PRAGMA table_info(exam_files)').fetchall()]); print([r[1] for r in conn.execute('PRAGMA table_info(exams)').fetchall()])"
```
Confirm focus_score, dependency_id, extracted_text, auditor_draft columns exist.
  </verify>
  <done>All four migration types applied: tasks gets focus_score + dependency_id, exam_files gets extracted_text + updated CHECK constraint with summary/sample_exam, exams gets auditor_draft. Server starts cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Uncap PDF extraction and save extracted_text at upload time</name>
  <files>backend/brain/syllabus_parser.py, backend/exams/routes.py</files>
  <action>
**syllabus_parser.py changes:**
- In `extract_text_from_pdf()`, change the `max_pages` parameter default from `5` to `None`.
- When `max_pages is None`, iterate ALL pages without a cap.
- Keep the parameter available for callers who still want a limit, but default behavior is full extraction.

**exams/routes.py upload handler changes:**
- In the file upload endpoint (the function handling POST to upload exam files), AFTER writing the file to disk and BEFORE the INSERT into exam_files:
  1. If the file is a PDF (check `filename.lower().endswith('.pdf')`), use `fitz.open(stream=content, filetype="pdf")` to extract text from ALL pages.
  2. Join all page texts with `"\n"`.
  3. Wrap in try/except — if extraction fails, log warning but set `extracted_text = None` (don't block upload).
  4. Add `extracted_text` to the INSERT statement for `exam_files`.

- Import `fitz` at the top of `routes.py` if not already imported.

**Important:** Use `run_in_executor` for the fitz extraction if the route is async, consistent with existing patterns. Check whether the upload handler is async — if so, wrap the synchronous fitz call.

Also check `exam_brain.py` for any `max_pages` cap when reading PDF content (the research mentions `max_pages=10` in exam_brain.py). Remove or default to None there too.
  </action>
  <verify>
Upload a test PDF via the API:
```bash
curl -X POST http://localhost:8000/exams/{exam_id}/files -F "file=@test.pdf" -F "file_type=syllabus"
```
Then query the DB to confirm extracted_text is populated:
```
python -c "import sqlite3; conn=sqlite3.connect('studyflow.db'); rows=conn.execute('SELECT filename, length(extracted_text) FROM exam_files ORDER BY id DESC LIMIT 1').fetchall(); print(rows)"
```
The extracted_text length should be > 0 for a valid PDF.
  </verify>
  <done>PDF extraction has no page cap. All uploaded PDFs get their full text stored in exam_files.extracted_text at upload time. Existing syllabus_parser.py callers still work (parameter is optional).</done>
</task>

</tasks>

<verification>
- Server starts without migration errors
- All new columns exist in the DB
- exam_files accepts 'summary' and 'sample_exam' file types without CHECK violation
- PDF uploads store extracted_text
- No regressions in existing upload flow
</verification>

<success_criteria>
Database schema is ready for the Split-Brain architecture. All four new columns exist. PDF extraction is uncapped. Upload handler saves extracted text. The foundation is set for Plans 02 and 03.
</success_criteria>

<output>
After completion, create `.planning/phases/17-split-brain-core-scheduler/17-01-SUMMARY.md`
</output>
